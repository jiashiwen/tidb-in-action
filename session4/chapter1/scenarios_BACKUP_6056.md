<<<<<<< HEAD
# 适用场景介绍

谈到场景我们不禁会想到各种各样的案例，若我们在这里罗列案例分析无疑会浪费大量篇幅，读者也未必会找到一条脉络来回答一个问题 “TiDB 是否适合我现在的工程环境？”。

我们不妨换个角度来考虑场景问题。所谓场景，首先要知道产品是什么，然后知道这个产品在什么情况下比较好用，再次看有没有相似的产品以及与该产品的主要区别。知道了以上这些，在面对具体的环境时，我们就可以判断产品是否合适，缩短选型的决策时间。

在本章最后我们会给出 TiDB 目前线上使用的一些经典案例，让读者有一些感性认识。

## TiDB 要解决什么问题
首先 TiDB 是一个数据库。加个几个定语， TiDB 是一个支持ACID的分布式关系型数据库。高度兼容Mysql协议。说起数据库的概念，在20年前大家的认识可能比较直观，大致的概念是--支持SQL的数据存储软件产品即RDBMS，到不是不存在其他类型的数据库而是单机关系型数据库掩盖了其他数据库的光芒。
随着互联网的发展，人们开始认识到传统单机关系型数据库的局限。数据的概念被泛化，如今支持数据存储和查询的软件系统都被叫做数据库。于是出现了文档数据库、时序数据库、分布式数据库、KV数据库等等。简而言之都是为了满足传统单机数据库扩展性的不足。很多新的数据库支持横向扩展的同时放弃了SQL支持，于是这类数据库有了个名字NOSQL。大部分NoSql产品为了扩展性牺牲了强一致性，开发者要自己实现业务出错时的补偿机制。没有标准的数据操作规范，程序员需要面对各种数据操作接口，增加了出错的概率。最近几年，人们开始意识到了标准的数据操作对于开发和维护的意义。简单来讲数据的发展趋势就是：SQL->NOSQL->NO,SQL。
回归并非回到原点而是螺旋上升，大家期待的一种兼容SQL、可以横向扩展并支持ACID的数据库。 TiDB 正是这种数据库的开源实现。如果您现在正在使用全球最流行的Mysql数据库，由于业务增长单机无法承载海量业务又不想让分库分表折磨。 TiDB 无疑是目前最好的选择，这也是 TiDB 经典的不能再经典的使用场景。泛化一点，需要数据层面横向扩展有需要兼容SQL和ACID的技术团队都可以尝试 TiDB 。配合pingcap公司自研的TiFlash列存引擎 TiDB 对于数仓及大数据业务也有很好的支持作用。

## TiDB 相似产品的横向比较
在解决传统单机RDBMS横向扩展问题过程中诞生了不少方案。有代表性的有以下几种
### sql-proxy方案
简而言之，sql-proxy方式是通过一个代理层来重写Sql实现数据分片与分片数据查询。使用已有的RDBMS产品作为数据存储。
方案特点：
* 根据业务手工对数据Hash分片
* SQL 不能跨维度 join、聚合、子查询
* 通过下游数据库主备方案保证数据高可用
* 集群扩展困难，数据在集群节点数量变更后需要根据hash算法进行数据再平衡
sql-proxy方案出现比较早，开源产品有mycat、shardingsphere等。以及云厂商推出的DRDS

### 利用共享存储的云原生数据库方案
随着云计算的兴起，云计算厂商利用高性能分布式存储共享数据库文件，在共享数据库文件的基础上启动多个数据库实例实现一写多读的数据库集群。
方案特点：
* 数据库存储与实例分离
* 一写多读，读写分离
* 通过网络广播写入节点cache嫌少存储开销
* 全托管，按需付费
aws 的aurora，阿里云的PolarDB属于这种方案实现

### google F1/spanner
google F1/spanner在分布式数据库领域大名鼎鼎。这里就不班门弄斧，说说方案特点
* 存储计算分离
* 自动调度各个节点数据均匀分布
* 数据多副本保障数据安全
* 通过“Percolator”算法模型实现ACID
 TiDB 是根据google F1/spanner论文实现的开源产品，并在多年实践中进行了很多工程优化工作。
在实际工作中sql-proxy方案属于过度系统，牵动业务改造大。目前采用这种架构的系统多属于2010's前有的遗留系统。
云原生数据库的共享存储方式类似于ORACEL Oracle Real Application Clusters(RAC)，本质上并没有实现数据库系统层面的数据分布式调度。这种方案依靠共享存储扩展存储容量，通过底层分布式存储实现数据多副本。托管模式意味着数据的物理位置存放于云厂商IDC。技术不透明，很少有云厂商对外公布实现细节。

## 典型场景
### 高并发 OLTP 
MySQL 是第一个广泛使用的开源关系型数据库，也是互联网业务数据库的事实标准。面对终端用户的迅猛增长，MySQL 数据库的架构方案很快就能面临支撑能力上的瓶颈。 TiDB 最初设想的使用场景就是通过计算存储分层的分布式设计实现对单机 MySQL 性能的突破。

在 NewSQL 还没有出现前，MySQL 遭遇业务迅速增长造成的瓶颈时，数据库架构的演化方向会选用分库分表。大部分情况下采用的方法是将高吞吐的大表按主键的 Hash 值切分（称为 Sharding），表上数据的分发、路由引入中间件进行处理。自下而上分为三层，分别 DB 层、中间层、应用层。分库分表方案部分解决了业务扩展的问题，但是另外一方面对开发和运维造成了巨大的压力。开发上需要业务提供切分维度，DDL 操作过程复杂，不能跨维度 Join / 聚合 / 子查询，不支持分布式事务，无法实现多维度的业务需求。运维上不能在线进行扩缩容，不能实现一致性的备份还原，难以实现异地容灾等。

|               类型               | 数据库中间件  / 分库分表 |   TiDB   |
| :------------------------------: | :----------------------: | :------: |
|        强一致的分布式事务        |          不支持          |   支持   |
|             水平扩展             |          不支持          |   支持   |
| 复杂查询   (JOIN/  GROUP BY/...) |          不支持          |   支持   |
|        无人工介入的高可用        |          不支持          |   支持   |
|            业务兼容性            |            低            |    高    |
|            多维度支持            |          不友好          |   友好   |
|           全局 ID 支持           |          不友好          |   友好   |
|             机器容量             |          很浪费          | 随需扩容 |

 TiDB 对分库分表是一种颠覆，她优雅地实现 MySQL 的扩展，解决分库分表方案中存在的诸多限制。通过 KV 存储层和 SQL 计算层两种模式的转换关系，解决了多维度的业务需求，支持复杂 SQL 查询，分布式事务模型能保证数据一致性，支持在线 DDL。基于 Region 的 Multi-Raft 设计，支持在线进行扩缩容，支持无人工介入的高可用等。

 TiDB 兼容 MySQL 生态，如连接协议、开发框架、ORM，适合主流的互联网业务开发风格。基于 MySQL 的业务简单到只需要修改数据源就能直连 TiDB 直接运行，原有的多维度业务需求的产品功能可以继续保留。当业务规模发生变化时， TiDB 可以在线地进行节点的横向扩展或收缩以适配业务的变化。


### 实时分析 
传统的数据架构设计上，每个数据库都有一个明显的角色身份，比如业务系统库，经营分析库，报表库，数据仓库等。使用 ETL 工具按照时间和数据获取策略将交易数据抽取到数据分析平台，以满足业务的分析需求。

![图片](/res/session4/chapter1/scenarios/etl.png)

受制于数据抽取方式的时间策略和分析平台的性能，业务部门最常抱怨的莫过于分析的时效性，实时分析的概念应运而生。在实时分析的领域，离线和在线的边界越来越模糊，一切数据皆服务化，一切分析皆在线化。数据的实时分析结果直接服务于业务，对系统处理延时提出了新的挑战。

在 TiDB 4.0 版本之前，很难在一个集群内既支持实时分析处理，又要支持高并发的在线交易。3.0 版本中的TiSpark 组件， 将 Spark 的计算能力嫁接到 TiDB 的存储引擎 TiKV 之上。由于 Spark 的计算模型重且资源消耗高，在没有资源隔离支持的情况下，通常会将存储引擎的处理能力消耗殆尽，实时分析和交易处理成为“鱼和熊掌不可兼得”。

在  TiDB 4.0 版本中，引入列存引擎 TiFlash，既加速了分析运算，又解决了资源隔离的问题。通过 Raft learner 机制，将行存数据复制出列存数据，使用 Label 实现资源访问的物理隔离。两种不同资源需求的场景实现共存，实时分析使用列存引擎 TiFlash ，交易处理使用行存引擎 TiKV。列存引擎 TiFlash 组件的加入，补完 TiDB HATP 版图中的缺失功能。

![图片](/res/session4/chapter1/scenarios/htap.png)

对于业务更具价值的是，Raft learner 机制能将行存引擎上最为新鲜的业务数据复制到 TiFlash 中，实现业务数据的实时分析，分析的结果可以回写到行存引擎，进而为实时数据服务提供更多的想象空间。熟悉 Oracle 的读者，对于报表系统使用 DataGuard 备库，计算结果写回主库的架构设计应该不陌生。 TiDB HTAP 架构的优势在于，在一套集群内通过扩展存储引擎组件的方式实现计算访问的灵活性，列式的存储不仅能够极大地加速分析场景的计算，同时交易场景可以利用列存实现类似索引的效果。在表级别实现灵活的配置方式，也免去整个过程中的人工介入减少大量的维护成本。


## 经典案例
在几百家公司实际使用 TiDB 的过程中积累了大量的案例，在这些案例中也许读者会找到进自己场景的影子或者案例本身就是你面对场景的 TiDB 解决方案。欢迎访问以下链接阅读这些经典案例。

经典案例汇总链接：[https://pingcap.com/cases-cn/](https://pingcap.com/cases-cn/)
=======


谈到场景我们不禁会想到各种各样的案例，单纯只是罗列案例无疑会浪费大量篇幅，读者也未必会找到一条脉络来回答一个问题“TiDB是否适合我现在的工程环境？”。我们不妨换个角度，所谓适用场景，首先要明确解决什么问题，然后考察产品的哪些特性有利于解决问题，有没有类似的方案并完成优缺点的对比，最后通过阅读实际案例增强感性认知。通过前述过程，在面对具体的环境时，我们就可以快速判断产品的适用情况，缩短选型的决策时间。在本节中我们会给出常见的场景描述和经典案例，让读者有更具体的认识。

# TiDB 要解决什么问题
在 20 年前，大家对数据库的认知可能比较单一，大致的概念是“支持 SQL 的数据存储软件产品”，基于等同于 RDBMS。单机关系型数据库如日中天，掩盖了其他类型数据库的光芒。随着互联网的发展，人们开始认识到传统单机关系型数据库的局限。数据的概念被泛化，如今支持数据存储和查询的软件系统都被叫做数据库，于是出现了文档数据库、时序数据库、分布式数据库、KV 数据库等等。简而言之，都是为了满足传统单机数据库扩展性的不足。很多新的数据库支持横向扩展的同时放弃了 SQL 接口和事务支持，于是这类数据库被称为 NoSQL。大部分 NoSQL 产品牺牲强一致性换取扩展性，开发者要自己实现业务出错时的补偿机制。没有标准的数据操作规范，程序员需要面对各种数据操作接口，增加了出错的概率。最近几年，人们开始意识到了标准的数据操作对于开发和维护的意义。简单来讲数据库的发展趋势就是：SQL -> NoSQL ->No only SQL。

回归并非回到原点而是螺旋上升，大家期待一种数据库，既兼容SQL，又可以横向扩展，并支持事务，这也是 NewSQL 的定义。TiDB 正是一款 NewSQL 数据库产品。如果你现在正在使用 MySQL 数据库，面对无法承载的海量业务又不想让分库分表折磨，TiDB 无疑是目前最好的选择，这也是 TiDB 最为经典的场景。泛化一点，需要在数据层面横向扩展又需要兼容 SQL 和事务 ACID 特点的技术团队都可以尝试 TiDB。

在进行 TiDB 产品适用场景的正式介绍之前，可以站在使用者的角度，来观察 TiDB 的最为突出的特质：

* 架构经过重新设计的可以横向扩展的 MySQL。
* Raft 多数派一致性协议实现数据的高可用和存储的灵活性。
* 不同计算场景的需求可以灵活地访问共存的行式和列式数据。
* 计算存储分离的分布式架构易于与云的弹性特质结合。
# 适用场景
## 高并发 OLTP 
MySQL 是第一个广泛使用的开源关系型数据库，也是互联网业务数据库的事实标准。面对终端用户的迅猛增长，MySQL 数据库的架构方案很快就面临承载能力上限。TiDB 最初设想的使用场景就是通过计算存储分层的分布式设计实现单机 MySQL 性能的突破。

在 NewSQL 还没有出现前，MySQL 遭遇业务迅速增长造成的瓶颈时，数据库架构的演化方向通常会选用分库分表。将高吞吐的大表按主键值的 Hash 值进行切分（称为 Sharding），表上数据的分发、路由引入中间件进行处理。自下而上分为三层，分别 DB 层、中间层、应用层。分库分表方案部分解决了业务扩展的问题，但是另外一方面对开发和运维造成巨大的压力。业务需要提供切分维度，不支持在线 DDL 操作，不能跨维度 Join / 聚合 / 子查询，不支持分布式事务，无法实现多维度的业务需求。业务程序从单机数据库迁移到分库分表方案时，通常要完成大量的开发适配改造。运维要面对较大的可用性压力，不能在线进行扩缩容，不能实现一致性的备份还原，难以实现异地容灾等。

 类型   | 分库分表数据库中间件    | TiDB   
---|---|---
 强一致的分布式事务   | 不支持   | 支持   
 水平扩展   | 不支持   | 支持   
 复杂查询 （JOIN、GROUP BY、...）  | 不支持   | 支持   
 无人工介入的高可用   | 不支持   | 支持   
 业务兼容性   | 低   | 高   
 多维度支持   | 不友好   | 友好   
 全局 ID 支持   | 不友好   | 友好   
 机器容量   | 很浪费   | 随需扩容   

TiDB 之于分库分表方案是一种颠覆，分布式架构优雅地实现了水平扩展，解决了诸多的开发限制。计算层二维表与存储层  KV 键值对两种模式的转换，支持多维度的业务需求和复杂 SQL 查询，支持在线 DDL。Percolator 事务模型能保证 ACID 。基于 Region 的 Multi-Raft 设计，支持在线进行扩缩容，支持无人工介入的高可用等。

TiDB 兼容 MySQL 的开发生态，基于 MySQL 的业务只需要修改数据源连接 TiDB 就能运行。当业务规模发生变化时，TiDB 可以在线地进行节点的横向扩展或收缩以适配业务的变化。在 TiDB 4.0 中，弹性调度特性能根据现有的热点统计，利用 K8s 容器编排平台上灵活的调度能力，自动化地扩展 TiKV Pod 并迁移热点 Region，实现对前台业务负载变化的快速响应。

案例参考

* TiDB 在知乎万亿量级业务数据下的实践和挑战

 [https://pingcap.com/cases-cn/user-case-zhihu/](https://pingcap.com/cases-cn/user-case-zhihu/)

* TiDB at 丰巢：尝鲜分布式数据库

[https://pingcap.com/cases-cn/user-case-fengchao/](https://pingcap.com/cases-cn/user-case-fengchao/)

## 实时分析 
传统的数据架构设计中，每个数据库都有一个明显的角色身份，比如业务系统库，经营分析库，报表库，数据仓库等。使用 ETL 工具按照时间和数据获取策略将交易数据抽取到数据分析平台，满足业务的分析需求。

![图片](https://uploader.shimo.im/f/mspUVxCnPzQNLNSu.png!thumbnail)

受制于数据抽取方式的时间策略和分析平台的性能，业务部门最常抱怨的莫过于分析时效性，实时分析的概念应运而生。在实时分析领域，离线和在线的边界越来越模糊，一切数据皆服务化，一切分析皆在线化。数据的实时分析结果直接服务于业务，这对系统处理延时提出了新的挑战。

在 TiDB 4.0 版本之前，很难在一个集群内既支持实时分析处理，又要支持高并发的在线交易。3.0 版本中的TiSpark 组件， 将 Spark 的计算能力嫁接到 TiDB 的存储引擎 TiKV 之上。由于  Spark 的计算模型重且资源消耗高，在没有资源隔离支持的情况下，通常会将存储引擎的处理能力消耗殆尽，实时分析和交易处理成为“鱼和熊掌不可兼得”。

在  TiDB 4.0 版本中，引入列存引擎 TiFlash，既加速了分析运算，又解决了资源隔离的问题。通过 Raft learner 机制，将行存数据复制出列存数据，使用 Engine tag 实现访问资源的物理隔离。两种不同资源需求的场景实现共存，实时分析使用列存引擎 TiFlash ，交易处理使用行存引擎 TiKV。列存引擎 TiFlash 组件的加入，补全 TiDB HATP 版图。

![图片](https://uploader.shimo.im/f/9RS3aDNJjeszK9gJ.png!thumbnail)

对于业务更具价值的是，Raft learner 机制能将行存引擎上最为新鲜的业务数据复制到 TiFlash 中，实现业务数据的实时分析，分析的结果可以回写到行存引擎，为实时数据服务提供更多的想象空间。熟悉 Oracle 的读者，对于报表系统使用 DataGuard 备库，计算结果写回主库的架构设计应该不陌生。TiDB HTAP 架构的优势在于，在一套集群内通过扩展存储引擎组件的方式实现计算访问的灵活性，列式的存储不仅能够极大地加速分析场景的计算，同时交易场景可以利用列存实现类似索引的效果。在表级别实现灵活的配置方式，也免去整个过程中的人工介入减少大量的维护成本。

案例参考

* TiDB / TiSpark 在易果集团实时数仓中的创新实践

[https://pingcap.com/cases-cn/user-case-yiguo/](https://pingcap.com/cases-cn/user-case-yiguo/)

## 多活场景
多个数据中心内部署业务系统组件，如数据库服务器、应用服务器，并组成一个有机的整体，用户能够接入任意一个数据中心的业务系统实现多活访问，能有效提高系统健壮性和业务流量承载能力。

多活场景最主要的难点在于业务系统需要在同一份数据上提供数据服务。假设多活数据中心共有 A、B、C 三个站点，结合业务对于数据一致性的要求，需要保证在 A 站点发生故障后，业务系统此前发生的操作，在另外两个站点上也能访问到。常见的多活基础架构方式有：

* 数据库集群结合裸光纤互连的存储容灾复制方案，比如 Oracle Extended RAC。
* 按站点进行应用数模设计结合数据复制的，比如 A 站点的记录号为奇数，B 站点的记录号为偶数，利用序列的步长避免记录操作冲突，同时使用 Oracle GoldenGate 进行双向复制。
* 在应用层设计共享中心和业务中心，终端用户绑定业务中心属主，当用户访问非属主业务中心时，共享中心自动实现用户的漫游和数据跨中心的数据访问。

以上的多活设计方式中，如果优先保证一致性就会影响性能，如果优先满足性能就需要在一致性性做妥协。

在 TiDB 的多活场景设计中，根据各个分布式组件的高可用机制实现多活部署。TiDB Server 属于无状态应用，类似 Web 服务器，在多个站点部署结合负载均衡设备实现高可用和多活访问。TiKV Server 和 PD Server 基于 Raft 多数派一致性协议实现高可用。TiKV Server 以 Region 为单位，按指定的数据副本数进行存储，属于 Multi-Raft 设计。在高可用设计上还引入 DC / Zone / Rack / Host 的四层标签体系和 Raft Leader 的 Reject 排斥策略，能灵活地指定在多个站点的数据副本分布和 IO 的流量导向，通常配置数据副本数的倍数台 TiKV 节点，以实现数据的均匀分布。数据写入时，只需要在延时较低的站点内写入足够的数据副本数量就可以返回写入成功，避免了性能和数据一致性的妥协。PD Server 属于单 Raft 组设计，节点数等于数据副本数，在多个站点均衡配置 3 个或者更多节点。

TiDB 的多活架构设计，不需要在应用层上做数模的特殊设计，实现原生的业务多活。Raft 的多数派一致性设计，既降低了多活的网络要求，又满足了数据的高可用要求。同时整个多活体制的高可用机制，均由底层体制自动完成，不需要人工介入和额外的操作流程。TiDB 4.0 的 Follower Read 特性，能实现同站点读取操作的亲和性，有效提高存储层的数据吞吐能力并降低跨站点的网络流量，进一步降低了网络成本。

* TiDB 在银行核心金融领域的研究与两地三中心实践

[https://pingcap.com/cases-cn/user-case-beijing-bank/](https://pingcap.com/cases-cn/user-case-beijing-bank/)

* 微众银行数据库架构演进及 TiDB 实践经验

[https://pingcap.com/cases-cn/user-case-webank/](https://pingcap.com/cases-cn/user-case-webank/)

### 经典案例
经过几百个用户实际使用，TiDB 产品积累了大量的案例。在这些案例中，也许读者会找到自己场景的影子，或者案例本身就是你面对场景的 TiDB 解决方案。欢迎访问以下链接阅读用户案例。

用户案例汇总链接（[https://pingcap.com/cases-cn/](https://pingcap.com/cases-cn/)）

>>>>>>> e98ff91c529304375a0240bb409f85036334dbb4
