# 适用场景介绍

谈到场景我们不禁会想到各种各样的案例，若我们在这里罗列案例分析无疑会浪费大量篇幅，读者也未必会找到一条脉络来回答一个问题 “TiDB 是否适合我现在的工程环境？”。

我们不妨换个角度来考虑场景问题。所谓场景，首先要知道产品是什么，然后知道这个产品在什么情况下比较好用，再次看有没有相似的产品以及与该产品的主要区别。知道了以上这些，在面对具体的环境时，我们就可以判断产品是否合适，缩短选型的决策时间。

在本章最后我们会给出 TiDB 目前线上使用的一些经典案例，让读者有一些感性认识。

### TiDB 要解决什么问题
首先 TiDB 是一个数据库。加个几个定语， TiDB 是一个支持ACID的分布式关系型数据库。高度兼容Mysql协议。说起数据库的概念，在20年前大家的认识可能比较直观，大致的概念是--支持SQL的数据存储软件产品即RDBMS，到不是不存在其他类型的数据库而是单机关系型数据库掩盖了其他数据库的光芒。
随着互联网的发展，人们开始认识到传统单机关系型数据库的局限。数据的概念被泛化，如今支持数据存储和查询的软件系统都被叫做数据库。于是出现了文档数据库、时序数据库、分布式数据库、KV数据库等等。简而言之都是为了满足传统单机数据库扩展性的不足。很多新的数据库支持横向扩展的同时放弃了SQL支持，于是这类数据库有了个名字NOSQL。大部分NoSql产品为了扩展性牺牲了强一致性，开发者要自己实现业务出错时的补偿机制。没有标准的数据操作规范，程序员需要面对各种数据操作接口，增加了出错的概率。最近几年，人们开始意识到了标准的数据操作对于开发和维护的意义。简单来讲数据的发展趋势就是：SQL->NOSQL->NO,SQL。
回归并非回到原点而是螺旋上升，大家期待的一种兼容SQL、可以横向扩展并支持ACID的数据库。 TiDB 正是这种数据库的开源实现。如果您现在正在使用全球最流行的Mysql数据库，由于业务增长单机无法承载海量业务又不想让分库分表折磨。 TiDB 无疑是目前最好的选择，这也是 TiDB 经典的不能再经典的使用场景。泛化一点，需要数据层面横向扩展有需要兼容SQL和ACID的技术团队都可以尝试 TiDB 。配合pingcap公司自研的TiFlash列存引擎 TiDB 对于数仓及大数据业务也有很好的支持作用。

### TiDB 相似产品的横向比较
在解决传统单机RDBMS横向扩展问题过程中诞生了不少方案。有代表性的有以下几种
#### sql-proxy方案
简而言之，sql-proxy方式是通过一个代理层来重写Sql实现数据分片与分片数据查询。使用已有的RDBMS产品作为数据存储。
方案特点：
* 根据业务手工对数据Hash分片
* SQL 不能跨维度 join、聚合、子查询
* 通过下游数据库主备方案保证数据高可用
* 集群扩展困难，数据在集群节点数量变更后需要根据hash算法进行数据再平衡
sql-proxy方案出现比较早，开源产品有mycat、shardingsphere等。以及云厂商推出的DRDS

#### 利用共享存储的云原生数据库方案
随着云计算的兴起，云计算厂商利用高性能分布式存储共享数据库文件，在共享数据库文件的基础上启动多个数据库实例实现一写多读的数据库集群。
方案特点：
* 数据库存储与实例分离
* 一写多读，读写分离
* 通过网络广播写入节点cache嫌少存储开销
* 全托管，按需付费
aws 的aurora，阿里云的PolarDB属于这种方案实现

#### google F1/spanner
google F1/spanner在分布式数据库领域大名鼎鼎。这里就不班门弄斧，说说方案特点
* 存储计算分离
* 自动调度各个节点数据均匀分布
* 数据多副本保障数据安全
* 通过“Percolator”算法模型实现ACID
 TiDB 是根据google F1/spanner论文实现的开源产品，并在多年实践中进行了很多工程优化工作。
在实际工作中sql-proxy方案属于过度系统，牵动业务改造大。目前采用这种架构的系统多属于2010's前有的遗留系统。
云原生数据库的共享存储方式类似于ORACEL Oracle Real Application Clusters(RAC)，本质上并没有实现数据库系统层面的数据分布式调度。这种方案依靠共享存储扩展存储容量，通过底层分布式存储实现数据多副本。托管模式意味着数据的物理位置存放于云厂商IDC。技术不透明，很少有云厂商对外公布实现细节。

#### 典型场景
##### 高并发 OLTP 
MySQL 是第一个广泛使用的开源关系型数据库，也是互联网业务数据库的事实标准。面对终端用户的迅猛增长，MySQL 数据库的架构方案很快就能面临支撑能力上的瓶颈。 TiDB 最初设想的使用场景就是通过计算存储分层的分布式设计实现对单机 MySQL 性能的突破。

在 NewSQL 还没有出现前，MySQL 遭遇业务迅速增长造成的瓶颈时，数据库架构的演化方向会选用分库分表。大部分情况下采用的方法是将高吞吐的大表按主键的 Hash 值切分（称为 Sharding），表上数据的分发、路由引入中间件进行处理。自下而上分为三层，分别 DB 层、中间层、应用层。分库分表方案部分解决了业务扩展的问题，但是另外一方面对开发和运维造成了巨大的压力。开发上需要业务提供切分维度，DDL 操作过程复杂，不能跨维度 Join / 聚合 / 子查询，不支持分布式事务，无法实现多维度的业务需求。运维上不能在线进行扩缩容，不能实现一致性的备份还原，难以实现异地容灾等。

|               类型               | 数据库中间件  / 分库分表 |   TiDB   |
| :------------------------------: | :----------------------: | :------: |
|        强一致的分布式事务        |          不支持          |   支持   |
|             水平扩展             |          不支持          |   支持   |
| 复杂查询   (JOIN/  GROUP BY/...) |          不支持          |   支持   |
|        无人工介入的高可用        |          不支持          |   支持   |
|            业务兼容性            |            低            |    高    |
|            多维度支持            |          不友好          |   友好   |
|           全局 ID 支持           |          不友好          |   友好   |
|             机器容量             |          很浪费          | 随需扩容 |

 TiDB 对分库分表是一种颠覆，她优雅地实现 MySQL 的扩展，解决分库分表方案中存在的诸多限制。通过 KV 存储层和 SQL 计算层两种模式的转换关系，解决了多维度的业务需求，支持复杂 SQL 查询，分布式事务模型能保证数据一致性，支持在线 DDL。基于 Region 的 Multi-Raft 设计，支持在线进行扩缩容，支持无人工介入的高可用等。

 TiDB 兼容 MySQL 生态，如连接协议、开发框架、ORM，适合主流的互联网业务开发风格。基于 MySQL 的业务简单到只需要修改数据源就能直连 TiDB 直接运行，原有的多维度业务需求的产品功能可以继续保留。当业务规模发生变化时， TiDB 可以在线地进行节点的横向扩展或收缩以适配业务的变化。


##### 实时分析 
传统的数据架构设计上，每个数据库都有一个明显的角色身份，比如业务系统库，经营分析库，报表库，数据仓库等。使用 ETL 工具按照时间和数据获取策略将交易数据抽取到数据分析平台，以满足业务的分析需求。

![图片](/res/session4/chapter1/scenarios/etl.png)

受制于数据抽取方式的时间策略和分析平台的性能，业务部门最常抱怨的莫过于分析的时效性，实时分析的概念应运而生。在实时分析的领域，离线和在线的边界越来越模糊，一切数据皆服务化，一切分析皆在线化。数据的实时分析结果直接服务于业务，对系统处理延时提出了新的挑战。

在 TiDB 4.0 版本之前，很难在一个集群内既支持实时分析处理，又要支持高并发的在线交易。3.0 版本中的TiSpark 组件， 将 Spark 的计算能力嫁接到 TiDB 的存储引擎 TiKV 之上。由于 Spark 的计算模型重且资源消耗高，在没有资源隔离支持的情况下，通常会将存储引擎的处理能力消耗殆尽，实时分析和交易处理成为“鱼和熊掌不可兼得”。

在  TiDB 4.0 版本中，引入列存引擎 TiFlash，既加速了分析运算，又解决了资源隔离的问题。通过 Raft learner 机制，将行存数据复制出列存数据，使用 Label 实现资源访问的物理隔离。两种不同资源需求的场景实现共存，实时分析使用列存引擎 TiFlash ，交易处理使用行存引擎 TiKV。列存引擎 TiFlash 组件的加入，补完 TiDB HATP 版图中的缺失功能。

![图片](/res/session4/chapter1/scenarios/htap.png)

对于业务更具价值的是，Raft learner 机制能将行存引擎上最为新鲜的业务数据复制到 TiFlash 中，实现业务数据的实时分析，分析的结果可以回写到行存引擎，进而为实时数据服务提供更多的想象空间。熟悉 Oracle 的读者，对于报表系统使用 DataGuard 备库，计算结果写回主库的架构设计应该不陌生。 TiDB HTAP 架构的优势在于，在一套集群内通过扩展存储引擎组件的方式实现计算访问的灵活性，列式的存储不仅能够极大地加速分析场景的计算，同时交易场景可以利用列存实现类似索引的效果。在表级别实现灵活的配置方式，也免去整个过程中的人工介入减少大量的维护成本。


### 经典案例
在几百家公司实际使用 TiDB 的过程中积累了大量的案例，在这些案例中也许读者会找到进自己场景的影子或者案例本身就是你面对场景的 TiDB 解决方案。欢迎访问一下链接阅读这些经典案例。

经典案例汇总链接
[https://pingcap.com/cases-cn/](https://pingcap.com/cases-cn/)
